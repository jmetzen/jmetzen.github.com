<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html lang="en" xml:lang="en">
<head>
<title>Jan Hendrik Metzen:  Accounting for Task-Difficulty in Active Multi-Task Robot Control Learning</title>
<link rel="StyleSheet" href="../pfrstyle.css" type="text/css">
<meta http-equiv="content-type" content="text/html; charset=ISO-8859-1">
</head>
<body>
<h1>Jan Hendrik Metzen's Publications</h1>
<p><big>&#8226;
  <a href="sort_default.html">Default Ordering</a> &#8226;
  <a href="sort_date.html">Sorted by Date</a> &#8226;
  <a href="class_type.html">Classified by Publication Type</a> &#8226;
  <a href="class_rescat.html">Classified by Research Category</a> &#8226;
  </big></p>
<h2> Accounting for Task-Difficulty in Active Multi-Task Robot Control Learning</h2>
<p class="citation"> Alexander Fabisch, <a href="http://www.informatik.uni-bremen.de/~jhm/"> Jan Hendrik Metzen</a>,  Mario Michael Krell, and  Frank Kirchner.  Accounting for Task-Difficulty in Active Multi-Task Robot Control Learning. <i> German Journal of Artificial Intelligence</i>, pp. 1&ndash;9,  Springer Berlin Heidelberg, May 2015.</p>
<h3>Download</h3>
<p><a href="http://dx.doi.org/10.1007/s13218-015-0363-2">[PDF]</a>&nbsp;<a href="http://dx.doi.org/10.1007/s13218-015-0363-2">[gzipped postscript]</a>&nbsp;<a href="http://dx.doi.org/10.1007/s13218-015-0363-2">[postscript]</a>&nbsp;<a href="http://dx.doi.org/10.1007/s13218-015-0363-2">[HTML]</a>&nbsp;</p>
<h3>Abstract</h3>
<p class="abstract"> Contextual policy search is a reinforcement learning approach for multi-task learning in the context of robot control learning. It can be used to learn versatilely applicable skills that generalize over a range of tasks specified by a context vector. In this work, we combine contextual policy search with ideas from active learning for selecting the task in which the next trial will be performed. Moreover, we use active training set selection for reducing detrimental effects of exploration in the sampling policy. A core challenge in this approach is that the distribution of the obtained rewards may not be directly comparable between different tasks. We propose the novel approach PUBSVE for estimating a reward baseline and investigate empirically on benchmark problems and simulated robotic tasks to which extent this method can remedy the issue of non-comparable reward.</p>
<a href="fabisch_accounting_2015.bib"><h3>BibTeX</h3></a><pre>@article{fabisch_accounting_2015,
	title = {Accounting for {Task}-{Difficulty} in {Active} {Multi}-{Task} {Robot} {Control} {Learning}},
	issn = {0933-1875, 1610-1987},
	url = {http://link.springer.com/article/10.1007/s13218-015-0363-2},
	doi = {10.1007/s13218-015-0363-2},
	abstract = {Contextual policy search is a reinforcement learning approach for multi-task learning in the context of robot control learning. It can be used to learn versatilely applicable skills that generalize over a range of tasks specified by a context vector. In this work, we combine contextual policy search with ideas from active learning for selecting the task in which the next trial will be performed. Moreover, we use active training set selection for reducing detrimental effects of exploration in the sampling policy. A core challenge in this approach is that the distribution of the obtained rewards may not be directly comparable between different tasks. We propose the novel approach PUBSVE for estimating a reward baseline and investigate empirically on benchmark problems and simulated robotic tasks to which extent this method can remedy the issue of non-comparable reward.},
	language = {en},
	urldate = {2015-05-05},
	number = {"Advances in Autonomous Learning"},
        publisher={Springer Berlin Heidelberg},
	journal = {German Journal of Artificial Intelligence},
	author = {Fabisch, Alexander and Metzen, Jan Hendrik and Krell, Mario Michael and Kirchner, Frank},
	month = may,
	year = {2015},
	keywords = {Active learning, Artificial Intelligence (incl. Robotics), Contextual policy search, Multi-task learning, Software Engineering/Programming and Operating Systems},
	pages = {1--9},
        bib2html_pubtype = {Journal},
        bib2html_rescat = {Reinforcement Learning}
}
</pre>
<hr width="100%" size="2">
<p><small>
 Generated by
 <a href="https://sourceforge.net/projects/bib2html/">bib2html.pl</a>
 (written by <a href="http://sourceforge.net/users/patstg/">Patrick Riley</a>
  ) on
  Sun Nov 27, 2016 21:02:02</small></p>
</body>
</html>
