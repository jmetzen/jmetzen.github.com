<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html lang="en" xml:lang="en">
<head>
<title>Jan Hendrik Metzen:  Active Contextual Policy Search</title>
<link rel="StyleSheet" href="../pfrstyle.css" type="text/css">
<meta http-equiv="content-type" content="text/html; charset=ISO-8859-1">
</head>
<body>
<h1>Jan Hendrik Metzen's Publications</h1>
<p><big>&#8226;
  <a href="sort_default.html">Default Ordering</a> &#8226;
  <a href="sort_date.html">Sorted by Date</a> &#8226;
  <a href="class_type.html">Classified by Publication Type</a> &#8226;
  <a href="class_rescat.html">Classified by Research Category</a> &#8226;
  </big></p>
<h2> Active Contextual Policy Search</h2>
<p class="citation"> Alexander Fabisch and <a href="http://www.informatik.uni-bremen.de/~jhm/"> Jan Hendrik Metzen</a>.  Active Contextual Policy Search. <i> Journal of Machine Learning Research</i>,  15:3371&ndash;3399, 2014.</p>
<h3>Download</h3>
<p><a href="./fabisch_active_2014.pdf">[PDF]1.9MB
  </a>&nbsp;</p>
<h3>Abstract</h3>
<p class="abstract"> We consider the problem of learning skills that are versatilely applicable. One popular approach for learning such skills is contextual policy search in which the individual tasks are represented as context vectors. We are interested in settings in which the agent is able to actively select the tasks that it examines during the learning process. We argue that there is a better way than selecting each task equally often because some tasks might be easier to learn at the beginning and the knowledge that the agent can extract from these tasks can be transferred to similar but more dificult tasks. The methods that we propose for addressing the task-selection problem model the learning process as a nonstationary multi-armed bandit problem with custom intrinsic reward heuristics so that the estimated learning progress will be maximized. This approach does neither make any assumptions about the underlying contextual policy search algorithm nor about the policy representation. We present empirical results on an artificial benchmark problem and a ball throwing problem with a simulated Mitsubishi PA-10 robot arm.</p>
<a href="fabisch_active_2014.bib"><h3>BibTeX</h3></a><pre>@article{fabisch_active_2014,
	title = {Active Contextual Policy Search},
	abstract = {We consider the problem of learning skills that are versatilely applicable. One popular approach for learning such skills is contextual policy search in which the individual tasks are represented as context vectors. We are interested in settings in which the agent is able to actively select the tasks that it examines during the learning process. We argue that there is a better way than selecting each task equally often because some tasks might be easier to learn at the beginning and the knowledge that the agent can extract from these tasks can be transferred to similar but more dificult tasks. The methods that we propose for addressing the task-selection problem model the learning process as a nonstationary multi-armed bandit problem with custom intrinsic reward heuristics so that the estimated learning progress will be maximized. This approach does neither make any assumptions about the underlying contextual policy search algorithm nor about the policy representation. We present empirical results on an artificial benchmark problem and a ball throwing problem with a simulated Mitsubishi {PA}-10 robot arm.},
	journal = {Journal of Machine Learning Research},
	author = {Fabisch, Alexander and Metzen, Jan Hendrik},
        year = {2014},     
        volume  = {15},
        pages   = {3371-3399},
        url     = {http://jmlr.org/papers/v15/fabisch14a.html},
        bib2html_pubtype = {Journal},
        bib2html_rescat = {Reinforcement Learning},
        Local-Url = "../files/fabisch14a.pdf"
}
</pre>
<hr width="100%" size="2">
<p><small>
 Generated by
 <a href="https://sourceforge.net/projects/bib2html/">bib2html.pl</a>
 (written by <a href="http://sourceforge.net/users/patstg/">Patrick Riley</a>
  ) on
  Sun Nov 27, 2016 21:03:03</small></p>
</body>
</html>
