<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html lang="en" xml:lang="en">
<head>
<title>Jan Hendrik Metzen:  Evolving Neural Networks for Online Reinforcement Learning</title>
<link rel="StyleSheet" href="../pfrstyle.css" type="text/css">
<meta http-equiv="content-type" content="text/html; charset=ISO-8859-1">
</head>
<body>
<h1>Jan Hendrik Metzen's Publications</h1>
<p><big>&#8226;
  <a href="sort_default.html">Default Ordering</a> &#8226;
  <a href="sort_date.html">Sorted by Date</a> &#8226;
  <a href="class_type.html">Classified by Publication Type</a> &#8226;
  <a href="class_rescat.html">Classified by Research Category</a> &#8226;
  </big></p>
<h2> Evolving Neural Networks for Online Reinforcement Learning</h2>
<p class="citation"><a href="http://www.informatik.uni-bremen.de/~jhm/"> Jan Hendrik Metzen</a>,  Mark Edgington,  Yohannes Kassahun, and  Frank Kirchner.  Evolving Neural Networks for Online Reinforcement Learning. In <i> Parallel Problem Solving from Nature -- PPSN X</i>, pp. 518&ndash;527, September 2008.</p>
<h3>Download</h3>
<p><a href="http://dx.doi.org/10.1007/978-3-540-87700-4_52">[HTML]</a>&nbsp;</p>
<h3>Abstract</h3>
<p class="abstract"> For many complex Reinforcement Learning problems with large and continuous state spaces, neuroevolution (the evolution of artificial neural networks) has achieved promising results. This is especially true when there is noise in sensor and/or actuator signals. These results have mainly been obtained in offline learning settings, where the training and evaluation phase of the system are separated. In contrast, in online Reinforcement Learning tasks where the actual performance of the systems during its learning phase matters, the results of neuroevolution are significantly impaired by its purely exploratory nature, meaning that it does not use (i.e. exploit) its knowledge of the performance of single individuals in order to improve its performance during learning. In this paper we describe modifications which significantly improve the online performance of the neuroevolutionary method Evolutionary Acquisition of Neural Topologies (EANT) and discuss the results obtained on two benchmark problems.</p>
<a href="MetzenPPSN2008.bib"><h3>BibTeX</h3></a><pre>@inproceedings{Metzen:PPSN:2008,
	title = {Evolving Neural Networks for Online Reinforcement Learning},
	url = {http://dx.doi.org/10.1007/978-3-540-87700-4_52},
	booktitle = {Parallel Problem Solving from Nature -- {PPSN} X},
	author = {Jan Hendrik Metzen and Mark Edgington and Yohannes Kassahun and Frank Kirchner},
	month = sep,
	year = {2008},
	pages = {518--527},
        abstract = {For many complex Reinforcement Learning problems with large and continuous state spaces, neuroevolution (the evolution of artificial neural networks) has achieved promising results. This is especially true when there is noise in sensor and/or actuator signals. These results have mainly been obtained in offline learning settings, where the training and evaluation phase of the system are separated. In contrast, in online Reinforcement Learning tasks where the actual performance of the systems during its learning phase matters, the results of neuroevolution are significantly impaired by its purely exploratory nature, meaning that it does not use (i.e. exploit) its knowledge of the performance of single individuals in order to improve its performance during learning. In this paper we describe modifications which significantly improve the online performance of the neuroevolutionary method Evolutionary Acquisition of Neural Topologies (EANT) and discuss the results obtained on two benchmark problems.},
        bib2html_pubtype = {Refereed Conference},
        bib2html_rescat = {Neuroevolution, Reinforcement Learning}
}
</pre>
<hr width="100%" size="2">
<p><small>
 Generated by
 <a href="https://sourceforge.net/projects/bib2html/">bib2html.pl</a>
 (written by <a href="http://sourceforge.net/users/patstg/">Patrick Riley</a>
  ) on
  Mon May 01, 2017 19:36:28</small></p>
</body>
</html>
